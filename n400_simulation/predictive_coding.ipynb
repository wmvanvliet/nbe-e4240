{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef5367b0-47ca-4f0f-ab5b-44e8739251df",
   "metadata": {},
   "source": [
    "# Using a predictive coding model to explore questions about pseudowords\n",
    "\n",
    "This notebook runs the model proposed in:\n",
    "\n",
    "Nour Eddine, S., Brothers, T., Wang, L., Spratling, M., & Kuperberg, G. R. (2024). A predictive coding model of the N400. Cognition, 246, 105755. https://doi.org/10.1016/j.cognition.2024.105755\n",
    "\n",
    "\n",
    "<img src=\"images/model.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8a68c7-fc2f-4d70-80d9-65385e632e3e",
   "metadata": {},
   "source": [
    "## Run this first\n",
    "\n",
    "First, run the cell below to load in the model and some visualization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92bceea-ca24-420e-aef0-3c9651cde376",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run simulation\n",
    "plt.ioff();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb37ee-ec00-43da-b618-e439cb7992b7",
   "metadata": {},
   "source": [
    "## Run a word through the model\n",
    "\n",
    "You can run a single word through the model by executing the cell below.\n",
    "It will take a moment as the model is run for 40 steps.\n",
    "\n",
    "### About the visualization\n",
    "The image below shows you the state of the units in each layer, at each step in the simulation, after the prediction error has been applied and propagated and such.\n",
    "In the interface below, you can press the `â–¶` button to start the animation.\n",
    "The animation shows you the state of three layers of the model (we don't care so much about the context layer in this exercise) as it evolves over time. Dark blue means a unit has a value of `0`, other shades of blue mean the unit is somewhat active, shades of red mean the unit is very active, white means that the unit is active with a value of exactly `1`.\n",
    "\n",
    "### Take a moment to answer these questions for yourself\n",
    " 1. How is the word encoded by the orthographic layer? Can you tell which word it is without looking at the code?\n",
    " 2. How is the word encoded in the lexical layer?\n",
    " 3. How is the word encoded in the semantic layer?\n",
    " 4. Is this word in the vocabulary of the model? How can you tell?\n",
    " 5. How does the activity of the units evolve over time in the three layers?\n",
    "    1. Which layer activates \"first\", which activates \"last\"? Why would that be?\n",
    "    2. At which point, in which layer, is there some ambiguity as to what word is being activated?\n",
    "    3. Is this ambiguity resolved at some point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2ba10-7aed-4cb1-b78f-6928d48aa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# word: the word to run through the model\n",
    "# plot: what aspect of the model to plot. Can be \"state\" \"prediction\" \"prederr\"\n",
    "# n_steps: how many steps to run the simulation for\n",
    "anim = run_model(word=\"more\", plot=\"state\", n_steps=40)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f78d9-05a8-4d0f-a83e-ec1c4c761d13",
   "metadata": {},
   "source": [
    "## Look at the downward predictions being made\n",
    "The cell below will show you a similar animation as before, but now shows the prediction of the state of each layer made by the layer above it.\n",
    "This is what gets compared to the actual state to produce prediction error.\n",
    "\n",
    "### Take a moment to answer these questions for yourself\n",
    "\n",
    "1. When do the predictions not match the actual state?\n",
    "2. Do the predictions (eventually) match the actual state?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916b29ee-fc26-4f5c-baaf-fc5628d21cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# word: the word to run through the model\n",
    "# plot: what aspect of the model to plot. Can be \"state\" \"prediction\" \"prederr\"\n",
    "# n_steps: how many steps to run the simulation for\n",
    "anim = run_model(word=\"more\", plot=\"prediction\", n_steps=40)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504341cf-27f4-4f61-a5fc-7893a27e2b61",
   "metadata": {},
   "source": [
    "## Look at the prediction errors being propagated upwards\n",
    "The cell below will show you a similar animation as before, but now shows the incoming prediction errors for each layer. Shades of blue means inhibition: these units will have less activation in the next step, shades of red mean excitation: these units will have more activation in the next step. White means the state of the unit will remain unchanged.\n",
    "\n",
    "### Take a moment to answer these questions for yourself\n",
    "1. What is happening in the lexical layer? Watch the inhibition and excitation closely.\n",
    "2. Does the model converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1436984-ecd2-46e1-a25f-7d0de90f8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "# word: the word to run through the model\n",
    "# plot: what aspect of the model to plot. Can be \"state\" \"prediction\" \"prederr\"\n",
    "# n_steps: how many steps to run the simulation for\n",
    "anim = run_model(word=\"more\", plot=\"prederr\", n_steps=40)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb55abc-fbb0-44e9-abf8-09a3960df9ec",
   "metadata": {},
   "source": [
    "## Run a pseudoword through the model\n",
    "\n",
    "Now run a pseudoword through the model.\n",
    "Think of a 4-letter English pseudoword and fill it into the code below where indicated.\n",
    "You can change the `plot=` parameter to make the animation show different things, as we did in the code cells above.\n",
    "\n",
    "### Answer these questions\n",
    "1. How is the pseudoword represented in the orthographic layer? (look at `plot=\"state\"`)\n",
    "2. How is the pseudoword represented in the lexical layer? (look at `plot=\"state\"`)\n",
    "3. How is the pseudoword represented in the semantic layer? (look at `plot=\"state\"`)\n",
    "4. Are the states of the layers being correctly predicted by the layers above them? (look at `plot=\"reconstruction\"`)\n",
    "5. Is there anything different about the prediction error, compared to normal words? (look at `plot=\"prederr\"`)\n",
    "6. Does the model converge?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e347d96-0519-4d07-b310-1f0c946b9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudoword = \"\"  # fill in your 4-letter pseudoword\n",
    "\n",
    "# Parameters:\n",
    "# word: the word to run through the model\n",
    "# plot: what aspect of the model to plot. Can be \"state\" \"prediction\" \"prederr\"\n",
    "# n_steps: how many steps to run the simulation for\n",
    "anim = run_model(word=pseudoword, plot=\"state\", n_steps=40)\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36514787-bfdd-4914-aeb1-a875c8f5375b",
   "metadata": {},
   "source": [
    "## What aspect of the model shall we use to represent the N400 evoked response?\n",
    "\n",
    "The N400 is an evoked response peaking roughly 400 ms after the presentation of a word. Here is how it looks in MEG data in response to actual words versus pseudowords:\n",
    "\n",
    "<img src=\"images/n400.svg\" width=300>\n",
    "\n",
    "The N400 is associated with lexico-semantic processes, so let's take the lexical and semantic layers as representing the N400.\n",
    "The cell below will run a list of words through the model, and computes the sum activation of the requested aspect of the model in those layers, resulting the a timecourse for each word.\n",
    "It will show the average timecourse across the words and for reference show the average timecourse of a bunch of real words.\n",
    "\n",
    "Think of a bunch of pseudowords (at least 10 would be good, more would be even better) and enter them in the code cell below in the indicated spot.\n",
    "\n",
    "Play around with plotting different aspects of the model: `plot=\"state\"` `plot=\"reconstruction\"` `plot=\"prederr\"`.\n",
    "Which aspect would be most suited for simulating evoked responses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f76f0d9-dc03-4ae3-bff2-8170e6a11ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudowords = [\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"]  # enter 10 4-letter pseudowords here\n",
    "\n",
    "# Parameters:\n",
    "# words: the list of words to run through the model\n",
    "# plot: what aspect of the model to plot. Can be \"state\" \"prediction\" \"prederr\"\n",
    "# n_steps: how many steps to run the simulation for\n",
    "run_model_batch(words=pseudowords_model, plot=\"state\", n_steps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
